<!DOCTYPE html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml">
  <head>
  <meta charset="utf-8">

  <title>Alain Mauri - Wildeng</title>
  <meta name="author" content="Alain Mauri">
  <meta name="description" content="Alain Mauri Senior Software Engineer and Team Leader in Sheffield - UK">
  <meta name="keywords" content="Software Engineer, Team Leader, Ruby, Kanban,Startups, Agile, Pair Programming, Web Development, Javascript, CSS" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.0/css/bootstrap.min.css">
  <link rel="stylesheet" href="/style.css">
  <link rel="shortcut icon" href="images/favicon.ico">
  <link rel="alternate" type="application/atom+xml" title="Atom Feed for alainmauri.eu" href="/feed.xml" />
  <link href="https://unpkg.com/aos@2.3.1/dist/aos.css" rel="stylesheet">
  <script src="https://unpkg.com/aos@2.3.1/dist/aos.js"></script>
</head>

  <body>
    <div class="post-main">
      <div class="blog-content">
          
  <h1>Local ai code assistant</h1>

<p class="post-author"><u>05 Jun 2025 - Wildeng</u></p>

<p><h4 id="disclaimer-because-the-readme-file-of-this-project-is-good-i-decided-to-duplicate-it-here-too">Disclaimer: Because the README file of this project is good, I decided to duplicate it here too.</h4>

<h2 id="build-your-personal-code-assistant">Build your personal code assistant</h2>

<p>This is a personal experiment to explore whether it’s possible to build a local, offline-first code assistant on top of an open-source language model.</p>

<p>The assistant maintains context in a <code class="language-plaintext highlighter-rouge">JSON</code> file, which is updated with each interaction — allowing the model to pick up where it left off.</p>

<h2 id="problem-context">Problem Context</h2>

<p>This started because I didn’t want to rely on commercial products like GitHub Copilot, nor be dependent on an internet connection. I wanted something that works even on a train or plane.
The second constraint is about my development environment. I’m a <a href="https://www.lunarvim.org">Lunarvim</a> user and occasionally use <a href="https://neovim.io">Neovim</a>, so my goal was to first create something pluggable in <code class="language-plaintext highlighter-rouge">Lunarvim</code> and then adapt it for use in plain <code class="language-plaintext highlighter-rouge">Neovim</code>.
The third constraint was focusing on the programming language. I’m mainly <code class="language-plaintext highlighter-rouge">Ruby</code> developer and occasionally <code class="language-plaintext highlighter-rouge">Python</code> so I’ve started by focusing on these two only.</p>

<h2 id="how-it-works-high-level">How it Works (High-level)</h2>

<ul>
  <li>Lua plugin captures code context from the buffer</li>
  <li>Lua calls Python script, passing selected code (or surrounding context)</li>
  <li>Python script prompts the model via Ollama and returns code suggestions</li>
  <li>Suggestions are rendered in a floating window (markdown) or inline (ghost text)</li>
</ul>

<h2 id="draft-implementation">Draft Implementation</h2>

<p>To make the agent run I had to do two things first:</p>

<ul>
  <li>Install <a href="https://github.com/ollama/ollama">Ollama</a> and pull my model of choice <a href="https://huggingface.co/deepseek-ai/deepseek-coder-1.3b-instruct">Deepseek Coder 1.3b</a>. The code is general enough to be used with other models, depending on your machine available resources.</li>
  <li>To keep stuff segregated I created a <code class="language-plaintext highlighter-rouge">Python</code> environment where all the libraries should be installed. It’s not strictly necessary though.</li>
</ul>

<p>The first iteration is the <code class="language-plaintext highlighter-rouge">get_suggestions.py</code> and <code class="language-plaintext highlighter-rouge">ai_assistant.lua</code>. These two work together as follows:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">ai_assistant.lua</code> digests what’s been selected in the current buffer and passed to it. It then calls <code class="language-plaintext highlighter-rouge">get_suggestions.py</code> which returns a markdown formatted suggestion for the selected code and displays everything in a floating window.</li>
  <li>I then added some line to my <code class="language-plaintext highlighter-rouge">Lunarvim</code> configuration file to trigger the process:</li>
</ul>

<div class="language-lua highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">-- Keymap to trigger the AI assistant</span>
<span class="n">lvim</span><span class="p">.</span><span class="n">keys</span><span class="p">.</span><span class="n">visual_mode</span><span class="p">[</span><span class="s2">"&lt;leader&gt;ai"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">":&lt;C-U&gt;lua require('ai.ai_assistant').run_code_assistant()&lt;CR&gt;"</span>
</code></pre></div></div>

<p>The second iteration is more close to what I had in mind and is made up by two other files <code class="language-plaintext highlighter-rouge">inline_suggest.lua</code> and <code class="language-plaintext highlighter-rouge">get_inline_suggestions.py</code>.</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">inline_suggest.lua</code> gets the position of the cursor and the context around it by going back 100 lines and forward 20. It then calls the <code class="language-plaintext highlighter-rouge">get_inline_suggestions.py</code> and displays the results using “virtual text” or “ghost text”.</li>
  <li><code class="language-plaintext highlighter-rouge">get_inline_suggestions.py</code> is much simpler than the previous one and tries to instruct the model by using a prompt  designed to keep the model concise and avoid excessive commentary. What I need is the code only, without comments or other kind of suggestions.</li>
</ul>

<p>To call this last bit I added to my config the following lines</p>

<div class="language-lua highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">vim</span><span class="p">.</span><span class="n">o</span><span class="p">.</span><span class="n">updatetime</span> <span class="o">=</span> <span class="mi">500</span>  <span class="c1">-- 500 ms (you can tweak this)</span>
<span class="n">vim</span><span class="p">.</span><span class="n">api</span><span class="p">.</span><span class="n">nvim_create_autocmd</span><span class="p">(</span><span class="s2">"CursorHold"</span><span class="p">,</span> <span class="p">{</span>
  <span class="n">pattern</span> <span class="o">=</span> <span class="p">{</span> <span class="s2">"*"</span> <span class="p">},</span>
  <span class="n">callback</span> <span class="o">=</span> <span class="k">function</span><span class="p">()</span>
    <span class="nb">require</span><span class="p">(</span><span class="s2">"ai.inline_suggest"</span><span class="p">).</span><span class="n">run</span><span class="p">()</span>
  <span class="k">end</span><span class="p">,</span>
<span class="p">})</span>
</code></pre></div></div>
<p>This creates an auto command that triggers when the cursor is idle for 500ms and interrogates the model.</p>

<p>This is just a basic idea and a work in progress. My hope is to finally have a product that works well enough for my purposes and that can let me use these new technologies without being to0 dependent on the big corporations.</p>

<h2 id="whats-next">What’s Next</h2>
<ul>
  <li>Support for more languages</li>
  <li>Configurable context window size?</li>
  <li>Automatic memory/context trimming?</li>
  <li>Model fine-tuning (eventually?)</li>
</ul>

<hr />

<h4 id="things-i-like---in-random-order">Things I like - in random order</h4>

<p>Experimenting with colours:
<br />
<img class="post-image" src="https://pxscdn.com/public/m/_v2/424813098022428933/4f692c7f1-e20d1b/uGQzULFTPkrW/F6CAF7vfAQMVvObfZIc4eNbfogHxTTR6YODmYhn2.jpg" loading="lazy" alt="The image captures a quiet urban scene on a bright day. In the foreground, delicate cherry blossoms frame the view, their pale petals glowing warmly in the light. A white car is parked in an almost empty lot, while a cyclist wearing a helmet rides along a path beside the trees. Behind them, tall buildings rise — a red brick one to the left and a modern glass tower to the right. The sky is striking, a surreal mix of turquoise and violet tones, adding a dreamlike mood. The street is calm, evoking a peaceful, almost cinematic pause in the city." />
<br />
<br />
Experimenting with cinematic style:
<br />
<img src="https://pxscdn.com/public/m/_v2/424813098022428933/4f692c7f1-e20d1b/8lMIcI8Kc88G/2mYqpFXjqPEPYodAmWVyVJ5OYWfq7q0ZnA05xx2W.jpg" loading="lazy" alt="A young person in dark clothing crosses a quiet city street beneath a pale, overcast sky. To the left, a brick building marked &quot;BBC Radio Sheffield&quot; stands beside a street sign and a tall clock. Straight ahead, a faded beige building is covered in graffiti, with bare trees and empty benches nearby. Traffic lights show green, but only a few people are visible—two figures walking away on the right pavement, and a few distant cars. The scene feels still and contemplative, with muted colors and cinematic black bars above and below, adding a sense of framing and introspective distance." width="1770" height="1080" onerror="this.onerror=null;this.src='/storage/no-preview.png'" class="post-image" />
<br /></p>

<hr />

<h4 id="todays-links">Today’s Links</h4>

<p>Nintendo Switch 2 is the upgrade of my dreams <a href="https://www.theguardian.com/games/2025/jun/03/nintendo-switch-2-release-mario-kart-world">The Guardian</a></p>

<p>As usual corporate greed is at the base of this race to deploy AI agents:</p>

<blockquote>
  <p>Make no mistake: We’ve talked to scores of CEOs at companies of various sizes and across many industries.
Every single one of them is working furiously to figure out when and how agents or other AI technology can displace human workers at scale.
The second these technologies can operate at a human efficacy level, which could be six months to several years from now, companies will shift from humans to machines.</p>
</blockquote>

<p>Dario Amodei on the future of AI and the job market <a href="https://www.axios.com/2025/05/28/ai-jobs-white-collar-unemployment-anthropic">Behind the Curtain: A white-collar bloodbath</a></p>

<p>20 years ago <a href="https://www.postcrossing.com/20years/meetups">Postcrossing</a> was started. You should get into it, it’s fun!!</p>

</p>

      </div>
      <footer>
          <hr>

<div>
  <a href="/index.html" >Home</a>
  <a href="/blog" >Posts</a>
  <a href="/digital_garden.html" >Digital Garden</a>
  <a href="/blogroll/" >Blogroll</a>
</div>
<hr>
<p class="copyright">&copy; 1971-2025 <b>Alain Mauri</b>.
<br>rev 45e32d966a869120444452d40c8516a17ebc7773 - 2025.06.05</p>
<br/><p class="copyright">Subscribe to my <a href="/feed.xml" title="RSS Feed for the blog">RSS Feed</a></p>

 
      </footer>
    </div>
  </body>
</html>
